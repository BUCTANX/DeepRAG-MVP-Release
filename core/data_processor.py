import os
import json
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

def process_pdf_to_training_data(pdf_path, output_json_path):
    """
    1. è¯»å– PDF
    2. åˆ‡åˆ†æ–‡æœ¬
    3. æ„é€ å¾®è°ƒæ ¼å¼ (Instruction Tuning Format)
    """
    print(f"ğŸ“„ Processing {pdf_path}...")
    
    # 1. åŠ è½½
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    
    # 2. åˆ‡åˆ† (ä¸ºäº†å¾®è°ƒï¼Œå—å¯ä»¥ç¨å¾®å¤§ä¸€ç‚¹ï¼Œæˆ–è€…æŒ‰æ®µè½)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)
    
    training_data = []
    
    # 3. æ„é€  QA å¯¹ (è¿™é‡Œæ¨¡æ‹Ÿä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼šè®©æ¨¡å‹å­¦ä¼šå¤è¿°çŸ¥è¯†)
    # åœ¨çœŸå®äº§å“ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ GPT-4 æå– QA
    for split in splits:
        text = split.page_content.strip()
        if len(text) < 50: continue # è·³è¿‡å¤ªçŸ­çš„
        
        # æ„é€ ä¸€æ¡â€œé˜…è¯»ç†è§£â€é£æ ¼çš„æŒ‡ä»¤
        item = {
            "instruction": "Please explain the following content in detail.",
            "input": text[:50] + "...", # æç¤ºè¯å–å¼€å¤´
            "output": text # è®©æ¨¡å‹å­¦ä¼šè¾“å‡ºè¿™æ®µçŸ¥è¯†
        }
        training_data.append(item)
        
        # å†æ„é€ ä¸€æ¡â€œçŸ¥è¯†é—®ç­”â€é£æ ¼ (æ¨¡æ‹Ÿ)
        item2 = {
            "instruction": "What information is provided in the document?",
            "input": "",
            "output": f"The document mentions: {text}"
        }
        training_data.append(item2)
        
    # ä¿å­˜
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… Generated {len(training_data)} training samples at {output_json_path}")
    return len(training_data)