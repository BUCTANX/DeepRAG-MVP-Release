import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from peft import PeftModel
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import PromptTemplate

class RAGEngine:
    def __init__(self, base_model_id="Qwen/Qwen2-1.5B-Instruct"):
        self.base_model_id = base_model_id
        self.tokenizer = None
        self.model = None
        self.llm = None
        self.vectorstore = None
        
    def load_model(self, adapter_path=None):
        """
        åŠ è½½æ¨¡å‹ (å¼ºåˆ¶ä½¿ç”¨ 4-bit é‡åŒ– + æŒ‡å®š GPUï¼Œé˜²æ­¢ Meta Tensor æŠ¥é”™)
        """
        print("ğŸ¤– Loading Base Model...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_id)
        
        # --- å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ 4-bit é‡åŒ–é…ç½® ---
        # è¿™ä¸ä»…çœæ˜¾å­˜ï¼Œè¿˜èƒ½é¿å… meta device æŠ¥é”™
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        # --- å…³é”®ä¿®æ”¹ï¼šdevice_map={"": "cuda"} ---
        # å¼ºåˆ¶æ‰€æœ‰å±‚éƒ½åœ¨ GPU ä¸Šï¼Œç¦æ­¢ accelerate åˆ‡åˆ†æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_id,
            quantization_config=bnb_config,
            device_map={"": "cuda"}, # æ˜¾å¼æŒ‡å®š GPUï¼Œæ‹’ç» auto
            torch_dtype=torch.float16
        )
        
        if adapter_path and os.path.exists(adapter_path):
            print(f"âœ¨ Loading Adapter from {adapter_path}...")
            # åŠ è½½ LoRA
            self.model = PeftModel.from_pretrained(base_model, adapter_path)
        else:
            print("âš ï¸ No adapter found, utilizing Base Model.")
            self.model = base_model
            
        # æ„å»º pipeline
        pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=300, # ç¨å¾®è°ƒå¤§ä¸€ç‚¹ï¼Œè®©å®ƒå¤šè¯´ç‚¹
            temperature=0.7
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)
        print("âœ… Model Loaded Successfully.")
        
    def build_index(self, pdf_path):
        """
        æ„å»º RAG ç´¢å¼•
        """
        print("ğŸ“š Indexing PDF...")
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        texts = splitter.split_documents(docs)
        
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-en-v1.5",
            model_kwargs={'device': 'cuda'},
            encode_kwargs={'normalize_embeddings': True}
        )
        self.vectorstore = FAISS.from_documents(texts, embeddings)
        print("âœ… Index built.")
        
    def chat(self, query):
        if not self.vectorstore:
            return "è¯·å…ˆä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“ã€‚"
            
        # RAG æ£€ç´¢
        docs = self.vectorstore.similarity_search(query, k=2)
        context = "\n".join([d.page_content for d in docs])
        
        # æ„é€  Prompt
        template = """Answer the question based on the context.
        
        Context: {context}
        
        Question: {question}
        
        Answer:"""
        prompt = PromptTemplate(template=template, input_variables=["context", "question"])
        final_prompt = prompt.format(context=context, question=query)
        
        # ç”Ÿæˆ
        res = self.llm.invoke(final_prompt)
        
        # æ¸…æ´—ç»“æœ (å»æ‰ prompt éƒ¨åˆ†)
        # æœ‰æ—¶å€™æ¨¡å‹ä¼šæŠŠ Prompt å¤è¿°ä¸€éï¼Œè¿™é‡Œåšä¸€ä¸ªç®€å•çš„æˆªæ–­
        if final_prompt in res:
            return res.split("Answer:")[-1].strip()
        return res.strip()